## Summary Notes

### Attention is All You Need
- **Focus**: Introduced the Transformer model for sequence transduction.
- **Key Insight**: Replaces recurrence and convolutions with self-attention mechanisms.
- **Result**: Significant improvements in translation tasks with reduced computational cost.
- **Why Important**: Revolutionized deep learning for NLP and beyond.

### A Relational Model of Data for Large Shared Data Banks
- **Focus**: Proposed the relational database model.
- **Key Insight**: Emphasized data independence and normal forms to reduce redundancy.
- **Result**: Basis for modern database systems.
- **Why Important**: Fundamental to structured data storage and querying.
